# Some utilities
def str2bool(v):
    return v.lower() in ("yes", "true", "t", "1")

# Load Config
config_yaml=read_yaml('config.yaml')

# Load local Envfile with env vars
local_env_file='%s/.env' % config.main_dir
print ('Loading local .env file=%s' % local_env_file)
load('ext://dotenv', 'dotenv')
dotenv(fn=local_env_file)

# Vars
everest_namespace = 'everest-system'
everest_monitoring_namespace = 'everest-monitoring'
pxc_operator_version = os.getenv('PXC_OPERATOR_VERSION', '1.16.1')
print('Using PXC operator version: %s' % pxc_operator_version)
psmdb_operator_version = os.getenv('PSMDB_OPERATOR_VERSION', '1.19.0')
print('Using PSMDB operator version: %s' % psmdb_operator_version)
pg_operator_version = os.getenv('PG_OPERATOR_VERSION', '2.5.0')
print('Using PG operator version: %s' % pg_operator_version)

# External resources set up
# uncomment the settings below and insert your k8s context & registry names
# to get your current context name run `kubectl config view`
#allow_k8s_contexts("gke_percona-everest_europe-west1-c_everest-dev")
#default_registry("us-central1-docker.pkg.dev/percona-everest/quickstart-docker-repo")

# Check for required env vars
backend_dir = os.path.abspath(os.getenv('EVEREST_BACKEND_DIR', '%s/../' % config.main_dir))
if not os.path.exists(backend_dir):
  fail('Backend dir does not exist: %s' % backend_dir)
print('Using backend dir: %s' % backend_dir)

frontend_dir = os.path.abspath(os.getenv('EVEREST_FRONTEND_DIR', '%s/../ui/' % config.main_dir))
if not os.path.exists(frontend_dir):
  fail('Frontend dir does not exist: %s' % frontend_dir)
print('Using frontend dir: %s' % frontend_dir)

cli_dir = os.path.abspath(os.getenv('EVEREST_CLI_DIR', '%s/..' % config.main_dir))
if not os.path.exists(cli_dir):
  fail('CLI dir does not exist: %s' % cli_dir)
print('Using CLI dir: %s' % cli_dir)

operator_dir = os.getenv('EVEREST_OPERATOR_DIR')
if not operator_dir:
  fail('EVEREST_OPERATOR_DIR must be set')
operator_dir = os.path.abspath(operator_dir)
if not os.path.exists(operator_dir):
  fail('Operator dir does not exist: %s' % operator_dir)
print('Using operator dir: %s' % operator_dir)

everest_chart_dir = os.getenv('EVEREST_CHART_DIR')
if not everest_chart_dir:
  fail('EVEREST_CHART_DIR must be set')
everest_chart_dir = os.path.abspath(everest_chart_dir)
if not os.path.exists(everest_chart_dir):
  fail('Chart dir does not exist: %s' % everest_chart_dir)
print('Using chart dir: %s' % everest_chart_dir)

# Check for Everest debug
everest_debug = str2bool(os.getenv('EVEREST_DEBUG', 'False'))
print('Using Everest debug: %s' % everest_debug)

# Check for Everest Operator debug
everest_operator_debug = str2bool(os.getenv('EVEREST_OPERATOR_DEBUG', 'False'))
print('Using Everest Operator debug: %s' % everest_operator_debug)

watch_settings(
    ignore=[
        '%s/**/*.tgz' % everest_chart_dir,
        '%s/**/*tmpcharts*' % everest_chart_dir,
    ]
)

# Ensure operator's kustomize is installed
local('[ -x %s/bin/kustomize ] || make -C %s kustomize' % (operator_dir, operator_dir), quiet=True)

# Ensure frontend repo is initialized
local('make -C %s init' % (frontend_dir), quiet=True)
# Ensure backend repo is initialized
local('make -C %s init' % (backend_dir), quiet=True)

# Build helm dependencies
local('HELM=%s/bin/helm make -C %s deps' % (backend_dir, everest_chart_dir), quiet=True)


# Create namespaces
load('ext://namespace', 'namespace_create', 'namespace_inject')
namespace_create(everest_namespace)
k8s_resource(
  objects=[
    '%s:namespace' % everest_namespace
  ],
  new_name='everest-namespace',
)
namespaces = sorted([namespace['name'] for namespace in config_yaml['namespaces']])
for namespace in namespaces:
  namespace_create(namespace)
k8s_resource(
  objects=[
    '%s:namespace' % namespace for namespace in namespaces
  ],
  new_name='namespaces',
)

for namespace in namespaces:
  local_resource('update-label-%s' % namespace, 
  cmd='kubectl label namespace %s --overwrite app.kubernetes.io/managed-by=everest' % namespace, 
  resource_deps = [
    'namespaces',
  ])

#################################
## Install DB engine operators ##
#################################

# PXC
pxc_operator_bundle_yaml = local(['curl', '-s', 'https://raw.githubusercontent.com/percona/percona-xtradb-cluster-operator/v%s/deploy/bundle.yaml' % pxc_operator_version], quiet=True)
pxc_namespaces = [namespace['name'] for namespace in config_yaml['namespaces'] if 'pxc' in namespace['operators']]
# We keep track of the CRD objects separately from the operator deployment so
# that we can make the deployment depend on the CRDs to avoid tilt thinking
# that there are conflicts with multiple CRDs.
if pxc_namespaces:
  k8s_resource(
    objects=[
      # The CRDs don't really get installed in a namespace, but since each
      # operator install is tied to a namespace, tilt uses a namespace to track
      # the install so we need to include one object per CRD per namespace.
      '%s:%s' % (crd, namespace) for namespace in pxc_namespaces for crd in [
              'perconaxtradbclusterbackups.pxc.percona.com:customresourcedefinition',
              'perconaxtradbclusterrestores.pxc.percona.com:customresourcedefinition',
              'perconaxtradbclusters.pxc.percona.com:customresourcedefinition'
          ]
    ],
    resource_deps = [
      'namespaces',
    ],
    new_name='pxc-crds',
    labels=["db-operators"]
  )
for namespace in pxc_namespaces:
  k8s_yaml(namespace_inject(pxc_operator_bundle_yaml, namespace))
  k8s_resource(
    workload='percona-xtradb-cluster-operator%s' % (':deployment:' + namespace if len(pxc_namespaces) > 1 else ''),
    objects=[
      'percona-xtradb-cluster-operator:serviceaccount:%s' % namespace,
      'percona-xtradb-cluster-operator:role:%s' % namespace,
      'service-account-percona-xtradb-cluster-operator:rolebinding:%s' % namespace,
    ],
    resource_deps = [
      'namespaces',
      'pxc-crds',
    ],
    new_name='pxc:%s' % namespace,
    labels=["db-operators"]
  )

# PSMDB
psmdb_operator_bundle_yaml = local(['curl', '-s', 'https://raw.githubusercontent.com/percona/percona-server-mongodb-operator/v%s/deploy/bundle.yaml' % psmdb_operator_version], quiet=True)
psmdb_namespaces = [namespace['name'] for namespace in config_yaml['namespaces'] if 'psmdb' in namespace['operators']]
# We keep track of the CRD objects separately from the operator deployment so
# that we can make the deployment depend on the CRDs to avoid tilt thinking
# that there are conflicts with multiple CRDs.
if psmdb_namespaces:
  k8s_resource(
    objects=[
      # The CRDs don't really get installed in a namespace, but since each
      # operator install is tied to a namespace, tilt uses a namespace to track
      # the install so we need to include one object per CRD per namespace.
      '%s:%s' % (crd, namespace) for namespace in psmdb_namespaces for crd in [
        'perconaservermongodbbackups.psmdb.percona.com:customresourcedefinition',
        'perconaservermongodbrestores.psmdb.percona.com:customresourcedefinition',
        'perconaservermongodbs.psmdb.percona.com:customresourcedefinition',
      ]
    ],
    resource_deps = [
      'namespaces',
    ],
    new_name='psmdb-crds',
    labels=["db-operators"]
  )
for namespace in psmdb_namespaces:
  k8s_yaml(namespace_inject(psmdb_operator_bundle_yaml, namespace))
  k8s_resource(
    workload='percona-server-mongodb-operator%s' % (':deployment:' + namespace if len(psmdb_namespaces) > 1 else ''),
    objects=[
      'percona-server-mongodb-operator:serviceaccount:%s' % namespace,
      'percona-server-mongodb-operator:role:%s' % namespace,
      'service-account-percona-server-mongodb-operator:rolebinding:%s' % namespace,
    ],
    resource_deps = [
      'namespaces',
      'psmdb-crds',
    ],
    new_name='psmdb:%s' % namespace,
    labels=["db-operators"]
  )

# PG
pg_operator_bundle_yaml = local(['curl', '-s', 'https://raw.githubusercontent.com/percona/percona-postgresql-operator/v%s/deploy/bundle.yaml' % pg_operator_version], quiet=True)
pg_namespaces = [namespace['name'] for namespace in config_yaml['namespaces'] if 'pg' in namespace['operators']]
# We keep track of the CRD objects separately from the operator deployment so
# that we can make the deployment depend on the CRDs to avoid tilt thinking
# that there are conflicts with multiple CRDs.
if pg_namespaces:
 k8s_resource(
   objects=[
     # The CRDs don't really get installed in a namespace, but since each
     # operator install is tied to a namespace, tilt uses a namespace to track
     # the install so we need to include one object per CRD per namespace.
     '%s:%s' % (crd, namespace) for namespace in pg_namespaces for crd in [
       'perconapgbackups.pgv2.percona.com:customresourcedefinition',
       'perconapgclusters.pgv2.percona.com:customresourcedefinition',
       'perconapgrestores.pgv2.percona.com:customresourcedefinition',
       'postgresclusters.postgres-operator.crunchydata.com:customresourcedefinition',
       'crunchybridgeclusters.postgres-operator.crunchydata.com:customresourcedefinition',
       'perconapgupgrades.pgv2.percona.com:customresourcedefinition',
       'pgadmins.postgres-operator.crunchydata.com:customresourcedefinition',
       'pgupgrades.postgres-operator.crunchydata.com:customresourcedefinition'
     ]
   ],
   resource_deps = [
     'namespaces',
   ],
   new_name='pg-crds',
   labels=["db-operators"]
 )
for namespace in pg_namespaces:
  k8s_yaml(namespace_inject(pg_operator_bundle_yaml, namespace))
  k8s_resource(
    workload='percona-postgresql-operator%s' % (':deployment:' + namespace if len(pg_namespaces) > 1 else ''),
    objects=[
      'percona-postgresql-operator:serviceaccount:%s' % namespace,
      'percona-postgresql-operator:role:%s' % namespace,
      'percona-postgresql-operator:rolebinding:%s' % namespace,
    ],
    resource_deps = [
      'namespaces',
      'pg-crds',
    ],
    new_name='pg:%s' % namespace,
    labels=["db-operators"]
  )

################################
### Deploy Everest Helm chart ##
################################

# First apply the CRDs since we will not apply them in the helm chart.
everest_crds = kustomize('%s/config/crd' % operator_dir)
k8s_yaml(everest_crds)

everest_yaml = helm(
    everest_chart_dir,
    name='everest',
    namespace=everest_namespace,
    skip_crds=True,
    set=[
        'dbNamespace.enabled=false',
        'upgrade.preflightChecks=false',
        'olm.install=false',
        'createMonitoringResources=false',
        'monitoring.crds.enabled=true',
        'monitoring.crds.plain=false',
        'monitoring.namespaceOverride=%s' % everest_monitoring_namespace,
        'server.initialAdminPassword=admin',
    ],
)
# we don't need the catalog source
everest_catalog_source_yaml, everest_yaml = filter_yaml(everest_yaml, kind='CatalogSource', name='everest-catalog')

# Patch Everest Server deployment for running with debugger
def patch_everest_debug(in_yaml):
  if not everest_debug:
    return in_yaml

  print('Patching Everest Server deployment for running with debugger')
  objects = decode_yaml_stream(in_yaml)
  for o in objects:
    if o.get('kind') == 'Deployment' and o.get('metadata').get('name') == 'everest-server':
      o['spec']['template']['spec']['securityContext']= {'runAsNonRoot': False}
      for c in o['spec']['template']['spec']['containers']:
        if c.get('name') == 'everest':
          c['ports'] = [{'containerPort': 8080}, {'containerPort': 40000}]
          # In debugger mode the process may be in paused state, need to avoid pod restart
          c['readinessProbe'] = None
          c['livenessProbe'] = None
          c['resources'] = None
  return encode_yaml_stream(objects)

# Patch Everest Operator deployment for running with debugger
def patch_everest_operator_debug(in_yaml):
  if not everest_operator_debug:
    return in_yaml

  print('Patching Everest operator deployment for running with debugger')
  objects = decode_yaml_stream(in_yaml)
  for o in objects:
    if o.get('kind') == 'Deployment' and o.get('metadata').get('name') == 'everest-operator':
      o['spec']['template']['spec']['securityContext']= {'runAsNonRoot': False}
      for c in o['spec']['template']['spec']['containers']:
        if c.get('name') == 'manager':
          c['ports'] = [{'containerPort': 40001}]
          # Need to disable security context for debugger
          c['securityContext'] = None
          # In debugger mode the process may be in paused state, need to avoid pod restart
          c['readinessProbe'] = None
          c['livenessProbe'] = None
          c['resources'] = None
  return encode_yaml_stream(objects)

everest_yaml=patch_everest_debug(everest_yaml)
everest_yaml=patch_everest_operator_debug(everest_yaml)
k8s_yaml(everest_yaml)

#################################
####### Monitoring stack ########
#################################

k8s_resource(
  objects=[
    '%s:namespace' % everest_monitoring_namespace
  ],
  new_name='everest-monitoring-namespace',
)
k8s_resource(
    workload='kube-state-metrics',
    objects=[
        'kube-state-metrics:serviceaccount:%s' % everest_monitoring_namespace,
        'kube-state-metrics:clusterrole',
        'kube-state-metrics:clusterrolebinding',
        'customresource-config-ksm:configmap'
    ],
    resource_deps = [
      'everest-monitoring-namespace',
    ],
    new_name='kube-state-metrics-deploy',
    labels=["everest-monitoring"]
)
k8s_resource(
    objects = [
        'vlogs.operator.victoriametrics.com:customresourcedefinition',
        'vmagents.operator.victoriametrics.com:customresourcedefinition',
        'vmalertmanagerconfigs.operator.victoriametrics.com:customresourcedefinition',
        'vmalertmanagers.operator.victoriametrics.com:customresourcedefinition',
        'vmalerts.operator.victoriametrics.com:customresourcedefinition',
        'vmauths.operator.victoriametrics.com:customresourcedefinition',
        'vmclusters.operator.victoriametrics.com:customresourcedefinition',
        'vmnodescrapes.operator.victoriametrics.com:customresourcedefinition',
        'vmpodscrapes.operator.victoriametrics.com:customresourcedefinition',
        'vmprobes.operator.victoriametrics.com:customresourcedefinition',
        'vmrules.operator.victoriametrics.com:customresourcedefinition',
        'vmscrapeconfigs.operator.victoriametrics.com:customresourcedefinition',
        'vmservicescrapes.operator.victoriametrics.com:customresourcedefinition',
        'vmsingles.operator.victoriametrics.com:customresourcedefinition',
        'vmstaticscrapes.operator.victoriametrics.com:customresourcedefinition',
        'vmusers.operator.victoriametrics.com:customresourcedefinition',

    ],
    new_name='vm-crds',
    labels=["everest-monitoring"]
)
k8s_resource(
    workload='vm-operator',
    objects = [
        'vm-operator:serviceaccount',
        'vm-operator:role',
        'vm-operator:clusterrole',
        "victoriametrics\\:admin:clusterrole",
        "victoriametrics\\:view:clusterrole",
        'vm-operator:clusterrolebinding',
        'vm-operator:rolebinding'

    ],
    resource_deps = [
      'everest-monitoring-namespace',
      'vm-crds',
    ],
    new_name='vm-operator-deploy',
    labels=["everest-monitoring"]
)

################################
####### Everest operator #######
################################

everest_operator_build_target='build'
everest_operator_target='dev'
everest_operator_entrypoint=["./manager"]
everest_operator_port_forwards=[]
if everest_operator_debug:
    everest_operator_build_target='build-debug'
    everest_operator_target='debug'
    everest_operator_entrypoint=[
        "/dlv",
        "--listen=:40001",
        "--headless=true",
        "--api-version=2",
        "--continue=true",
        "--accept-multiclient=true",
        "exec",
        "/home/everest/manager",
        "--",
    ]
    everest_operator_port_forwards=[
        port_forward(40001, 40001, 'Everest Operator debugger'),
    ]

# Build the Everest operator manager locally to take advantage of the go cache
local_resource(
  'operator-build',
  'GOOS=linux make -C %s %s' % (operator_dir, everest_operator_build_target),
  deps=[
    'go.mod',
    'go.sum',
    '%s/api' % operator_dir,
    '%s/internal' % operator_dir,
    '%s/cmd' % operator_dir,
  ],
  ignore=['%s/api/*/zz_generated.deepcopy.go' % operator_dir],
  labels=["everest-operator"]
)

# Live update the Everest operator manager without generating a new pod
load('ext://restart_process', 'docker_build_with_restart')
docker_build_with_restart('perconalab/everest-operator',
  context='%s' % operator_dir,
  dockerfile='./everest-operator.Dockerfile',
  target=everest_operator_target,
  entrypoint=everest_operator_entrypoint,
  only=['%s/bin/manager' % operator_dir],
  live_update=[
    sync('%s/bin/manager' % operator_dir, '/home/everest/manager'),
  ]
)

k8s_resource(
    objects=[
        'backupstorages.everest.percona.com:customresourcedefinition',
        'databaseclusterbackups.everest.percona.com:customresourcedefinition',
        'databaseclusterrestores.everest.percona.com:customresourcedefinition',
        'databaseclusters.everest.percona.com:customresourcedefinition',
        'databaseengines.everest.percona.com:customresourcedefinition',
        'monitoringconfigs.everest.percona.com:customresourcedefinition',
    ],
    new_name='everest-crds',
    labels=["everest-operator"]
)
k8s_resource(
    workload='everest-operator',
    objects=[
    'everest-operator-controller-manager:serviceaccount:%s' % everest_namespace,
    'everest-operator-leader-election-role:role:%s' % everest_namespace,
    'everest-operator-local:rolebinding:%s' % everest_namespace,
    'everest-operator-metrics-reader:clusterrole',
    'everest-operator-metrics-auth-role:clusterrole',
    'everest-operator-metrics-auth-rolebinding:clusterrolebinding',
    'everest-operator-manager-role:clusterrole',
    'everest-operator-leader-election-rolebinding:rolebinding',
    'everest-operator-manager-rolebinding:clusterrolebinding'

    ],
    resource_deps = [
      'everest-namespace',
      'everest-crds',
    ],
    port_forwards=everest_operator_port_forwards,
    new_name='everest-operator-deploy',
    labels=["everest-operator"]
)

#################################
############ Everest ############
#################################

# Build frontend
local_resource(
  'frontend-build',
  'make -C %s build EVEREST_OUT_DIR=%s/public/dist' % (frontend_dir, backend_dir),
  deps=['%s/apps' % frontend_dir],
  ignore=[
    '%s/apps/*/dist' % frontend_dir,
    '%s/apps/*/.turbo' % frontend_dir,
  ],
  labels=["everest"]
)

# Live update the Everest container without generating a new pod
everest_build_target='build'
everest_target='dev'
everest_entrypoint=["./everest-api"]
everest_port_forwards=[
    port_forward(8080, 8080, 'Everest Server API'),
]
if everest_debug:
    everest_build_target='build-debug'
    everest_target='debug'
    everest_entrypoint=[
        "/dlv",
        "--listen=:40000",
        "--headless=true",
        "--api-version=2",
        "--continue=true",
        "--accept-multiclient=true",
        "exec",
        "/home/everest/everest-api",
        "--",
    ]
    everest_port_forwards=[
        port_forward(40000, 40000, 'Everest Server debugger'),
        port_forward(8080, 8080, 'Everest Server API'),
    ]

# Build backend
local_resource(
    'backend-build',
    'GOOS=linux make -C %s %s' % (backend_dir, everest_build_target),
    deps=[
        'go.mod',
        'go.sum',
        '%s/api' % backend_dir,
        '%s/cmd' % backend_dir,
        '%s/pkg' % backend_dir,
        '%s/public' % backend_dir,
        '%s/internal' % backend_dir,
        ],
    ignore=[
        frontend_dir,
        '%s/cmd/cli' % backend_dir,
        '%s/pkg/cli' % backend_dir,
        ],
    labels=["everest"]
)

docker_build_with_restart('perconalab/everest',
  context='%s' % backend_dir,
  dockerfile='./everest.Dockerfile',
  target=everest_target,
  entrypoint=everest_entrypoint,
  only=['%s/bin/everest' % backend_dir],
  live_update=[
    sync('%s/bin/everest' % backend_dir, '/home/everest/everest-api'),
  ]
)

k8s_resource(
    workload='everest-server',
    objects=[
    'everest-accounts:secret:%s' % everest_namespace,
    'everest-jwt:secret:%s' % everest_namespace,
    'everest-rbac:configmap:%s' % everest_namespace,
    'everest-settings:configmap:%s' % everest_namespace,
    'everest-admin-role:role:%s' % everest_namespace,
    'everest-admin-cluster-role:clusterrole',
    'everest-admin-cluster-role-binding:clusterrolebinding',
    'everest-admin:serviceaccount:%s' % everest_namespace,
    'everest-admin-role-binding:rolebinding'
    ],
    resource_deps = [
      'everest-namespace',
    ],
    port_forwards=everest_port_forwards,
    new_name='everest-server-deploy',
    labels=["everest"]
)
